{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning LightGBM hyperparameters (scikit-learn API)\n",
    "\n",
    "This example demonstrates how to use FCVOpt to tune LightGBM hyperparameters for a binary classification task.\n",
    "\n",
    "Key Features:\n",
    "- Uses `SklearnCVObj` to wrap LightGBM's scikit-learn API\n",
    "- Demonstrates FCVOpt's hierarchical Gaussian process for efficient cross-validation\n",
    "- Shows how to define a hyperparameter search space with appropriate scales\n",
    "\n",
    "Requirements: `lightgbm` must be installed via `pip install lightgbm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Set OpenMP threads to 1 to avoid threading conflicts on MacOS\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# FCVOpt imports\n",
    "from fcvopt.crossvalidation import SklearnCVObj\n",
    "from fcvopt.optimizers import FCVOpt\n",
    "from fcvopt.configspace import ConfigurationSpace\n",
    "from ConfigSpace import Integer, Float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data\n",
    "\n",
    "We create a synthetic binary classification dataset with class imbalance to simulate a realistic scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features matrix: (2000, 25)\n",
      "Class distribution: [1796  204]\n"
     ]
    }
   ],
   "source": [
    "# Generate binary classification dataset with class imbalance (90% vs 10%)\n",
    "# Using 2000 samples, 25 features (5 informative, 10 redundant)\n",
    "X, y = make_classification(\n",
    "    n_samples= 2000,\n",
    "    n_features= 25,\n",
    "    n_informative= 5,\n",
    "    n_redundant=10,\n",
    "    n_classes= 2, n_clusters_per_class= 2,\n",
    "    weights=[0.9, 0.1], # imbalanced data,\n",
    "    random_state=23\n",
    ")\n",
    "\n",
    "print(f\"Shape of features matrix: {X.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameter Search Space\n",
    "\n",
    "We'll tune key LightGBM hyperparameters using appropriate ranges and scales:\n",
    "\n",
    "| Hyperparameter | Range | Scale | Description |\n",
    "|----------------|-------|-------|-------------|\n",
    "| `n_estimators` | [50, 1000] | log | Number of boosting rounds (trees) |\n",
    "| `learning_rate` | [1e-3, 0.25] | log | Shrinkage rate for updates |\n",
    "| `num_leaves` | [2, 128] | log | Maximum number of leaves per tree |\n",
    "| `min_data_in_leaf` | [2, 100] | log | Minimum samples required in a leaf |\n",
    "| `colsample_bytree` | [0.05, 1.0] | log | Fraction of features used per tree |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration space object:\n",
      "  Hyperparameters:\n",
      "    colsample_bytree, Type: UniformFloat, Range: [0.05, 1.0], Default: 0.22360679775, on log-scale\n",
      "    learning_rate, Type: UniformFloat, Range: [0.001, 0.25], Default: 0.0158113883008, on log-scale\n",
      "    min_data_in_leaf, Type: UniformInteger, Range: [2, 100], Default: 14, on log-scale\n",
      "    n_estimators, Type: UniformInteger, Range: [50, 1000], Default: 224, on log-scale\n",
      "    num_leaves, Type: UniformInteger, Range: [2, 128], Default: 16, on log-scale\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create configuration space for hyperparameter search\n",
    "config = ConfigurationSpace()\n",
    "\n",
    "# Add hyperparameters with appropriate ranges and scales\n",
    "# Note: LightGBM sklearn API uses 'num_round' -> number of estimators\n",
    "config.add([\n",
    "    Integer('n_estimators', bounds=(50, 1000), log=True),\n",
    "    Float('learning_rate', bounds=(1e-3, 0.25), log=True),\n",
    "    Integer('num_leaves', bounds=(2, 128), log=True),\n",
    "    Integer('min_data_in_leaf', bounds=(2, 100), log=True),\n",
    "    Float('colsample_bytree', bounds=(0.05, 1), log=True),\n",
    "])\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Cross-Validation Objective\n",
    "\n",
    "We wrap the LightGBM classifier in a `SklearnCVObj` that:\n",
    "- Evaluates models using 10-fold stratified cross-validation\n",
    "- Uses ROC-AUC as the performance metric (converted to a loss: 1 - AUC)\n",
    "- Handles the model training and evaluation for each hyperparameter configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created CV objective with 10 folds\n",
      "Number of CV folds: 10\n",
      "Training samples: 2000\n",
      "Features: 25\n"
     ]
    }
   ],
   "source": [
    "# Define loss metric: we want to maximize AUC, so we minimize (1 - AUC)\n",
    "def auc_loss(y_true, y_pred):\n",
    "    return 1 - roc_auc_score(y_true, y_pred)\n",
    "\n",
    "# Create CV objective that wraps the LightGBM classifier\n",
    "cv_obj = SklearnCVObj(\n",
    "    estimator=lgb.LGBMClassifier(objective=\"binary\", verbosity=-1),  # Suppress LightGBM output\n",
    "    X=X, \n",
    "    y=y,\n",
    "    loss_metric=auc_loss,\n",
    "    needs_proba=True,\n",
    "    task='classification',\n",
    "    n_splits=10,  # 10-fold cross-validation\n",
    "    rng_seed=42,\n",
    "    stratified=True  # Use stratified splits to preserve class distribution\n",
    ")\n",
    "\n",
    "print(f\"Created CV objective with {cv_obj.cv.get_n_splits()} folds\")\n",
    "\n",
    "print(f\"Number of CV folds: {cv_obj.cv.get_n_splits()}\")\n",
    "print(f\"Training samples: {len(cv_obj.y)}\")\n",
    "print(f\"Features: {cv_obj.X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Hyperparameter Optimization with FCVOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates evaluated.....: 50\n",
      "Observed obj at incumbent..........: 0.0347222\n",
      "Estimated obj at incumbent.........: 0.0648997\n",
      "\n",
      " Best Configuration at termination:\n",
      " Configuration(values={\n",
      "  'colsample_bytree': 0.4551726039474,\n",
      "  'learning_rate': 0.0387740433153,\n",
      "  'min_data_in_leaf': 10,\n",
      "  'n_estimators': 123,\n",
      "  'num_leaves': 32,\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Initialize FCVOpt optimizer\n",
    "optimizer = FCVOpt(\n",
    "    obj=cv_obj.cvloss,  # The cross-validation objective function\n",
    "    n_folds=cv_obj.cv.get_n_splits(),  # Total number of folds (10)\n",
    "    config=config,  # Search space definition\n",
    "    acq_function='LCB',  # Lower Confidence Bound acquisition function\n",
    "    tracking_dir='./hpt_opt_runs/',  # Directory for saving optimization results\n",
    "    experiment='lgb_sklearn_tuning',  # Experiment name for tracking\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# Run optimization for 50 BO iterations\n",
    "best_conf = optimizer.optimize(n_trials= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold CV Loss: 0.0665\n",
      "10-fold CV ROC-AUC: 0.9335\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best configuration found by FCVOpt\n",
    "# Convert loss back to AUC for easier interpretation (loss = 1 - AUC)\n",
    "best_cv_loss = cv_obj(best_conf)\n",
    "best_cv_auc = 1 - best_cv_loss\n",
    "\n",
    "print(f\"10-fold CV Loss: {best_cv_loss:.4f}\")\n",
    "print(f\"10-fold CV ROC-AUC: {best_cv_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcvopt_test",
   "language": "python",
   "name": "fcvopt_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
