{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending CVObjective for Custom Models\n",
    "\n",
    "This notebook demonstrates how to extend the `CVObjective` base class to work with models that don't have a scikit-learn compatible API. We'll use LightGBM's native Python API as an example.\n",
    "\n",
    "When to extend CVObjective:\n",
    "- Your model doesn't follow scikit-learn's `fit`/`predict` interface\n",
    "- You need custom training logic (e.g., early stopping with validation sets)\n",
    "- You want fine-grained control over the training process\n",
    "\n",
    "When to use SklearnCVObj instead:\n",
    "- Your model follows scikit-learn's API (has `fit` and `predict` methods)\n",
    "- Examples: `RandomForestClassifier`, `XGBClassifier`, `LGBMClassifier`\n",
    "\n",
    "Requirements: `lightgbm` must be installed prior to running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Set OpenMP threads to 1 to avoid threading conflicts on MacOS\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# FCVOpt imports\n",
    "from fcvopt.crossvalidation import CVObjective\n",
    "from fcvopt.optimizers import FCVOpt\n",
    "from fcvopt.configspace import ConfigurationSpace\n",
    "from ConfigSpace import Integer, Float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data\n",
    "\n",
    "We use the same dataset as in the sklearn API example for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features matrix: (2000, 25)\n",
      "Class distribution: [1796  204]\n"
     ]
    }
   ],
   "source": [
    "# Generate binary classification dataset with class imbalance (90% vs 10%)\n",
    "# Using 2000 samples, 25 features (5 informative, 10 redundant)\n",
    "X, y = make_classification(\n",
    "    n_samples= 2000,\n",
    "    n_features= 25,\n",
    "    n_informative= 5,\n",
    "    n_redundant=10,\n",
    "    n_classes= 2, n_clusters_per_class= 2,\n",
    "    weights=[0.9, 0.1], # imbalanced data,\n",
    "    random_state=23\n",
    ")\n",
    "\n",
    "print(f\"Shape of features matrix: {X.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Hyperparameter Search Space\n",
    "\n",
    "We use the same hyperparameter space as in the sklearn API example:\n",
    "\n",
    "| Hyperparameter | Range | Scale | Description |\n",
    "|----------------|-------|-------|-------------|\n",
    "| `num_round` | [50, 1000] | log | Number of boosting rounds (trees) |\n",
    "| `learning_rate` | [1e-3, 0.25] | log | Shrinkage rate for updates |\n",
    "| `num_leaves` | [2, 128] | log | Maximum number of leaves per tree |\n",
    "| `min_data_in_leaf` | [2, 100] | log | Minimum samples required in a leaf |\n",
    "| `colsample_bytree` | [0.05, 1.0] | log | Fraction of features used per tree |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration space object:\n",
      "  Hyperparameters:\n",
      "    colsample_bytree, Type: UniformFloat, Range: [0.05, 1.0], Default: 0.22360679775, on log-scale\n",
      "    learning_rate, Type: UniformFloat, Range: [0.001, 0.25], Default: 0.0158113883008, on log-scale\n",
      "    min_data_in_leaf, Type: UniformInteger, Range: [2, 100], Default: 14, on log-scale\n",
      "    num_leaves, Type: UniformInteger, Range: [2, 128], Default: 16, on log-scale\n",
      "    num_round, Type: UniformInteger, Range: [50, 1000], Default: 224, on log-scale\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create configuration space for hyperparameter search\n",
    "config = ConfigurationSpace()\n",
    "\n",
    "# Add hyperparameters with appropriate ranges and scales\n",
    "config.add([\n",
    "    Integer('num_round', bounds=(50, 1000), log=True),\n",
    "    Float('learning_rate', bounds=(1e-3, 0.25), log=True),\n",
    "    Integer('num_leaves', bounds=(2, 128), log=True),\n",
    "    Integer('min_data_in_leaf', bounds=(2, 100), log=True),\n",
    "    Float('colsample_bytree', bounds=(0.05, 1), log=True)\n",
    "])\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend CVObjective for LightGBM Native API\n",
    "\n",
    "To create a custom CV objective, extend the `CVObjective` base class and implement the `fit_and_test` method.\n",
    "\n",
    "Key responsibilities:\n",
    "1. `CVObjective` base class: Handles the CV loop, fold splitting, and aggregation\n",
    "2. Your `fit_and_test` method: Trains and evaluates on a SINGLE fold\n",
    "\n",
    "The `fit_and_test` method receives train/test indices for one fold and should return a scalar loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGBMCVObj(CVObjective):\n",
    "    \"\"\"\n",
    "    Custom CVObjective for LightGBM native API.\n",
    "    \n",
    "    This demonstrates how to extend CVObjective for models that don't follow\n",
    "    scikit-learn's API. The parent class handles cross-validation, while this \n",
    "    class focuses on training/evaluating a single fold.\n",
    "    \"\"\"    \n",
    "    def fit_and_test(self, params, train_index, test_index):\n",
    "        \"\"\"\n",
    "        Train and evaluate model on a SINGLE fold.\n",
    "        \n",
    "        This method is called by CVObjective for each fold during cross-validation.\n",
    "        Do NOT loop over folds here - that's handled by the parent class.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        params : dict\n",
    "            Hyperparameters to evaluate (e.g., {'num_round': 100, 'learning_rate': 0.1})\n",
    "        train_index : array-like\n",
    "            Indices for training data in this fold\n",
    "        test_index : array-like\n",
    "            Indices for test data in this fold\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Loss value for this fold (lower is better)\n",
    "        \"\"\"\n",
    "        # Step 1: Extract data for this fold\n",
    "        # Handle both DataFrame and ndarray formats\n",
    "        if isinstance(self.X, pd.DataFrame):\n",
    "            X_train = self.X.iloc[train_index]\n",
    "            X_test = self.X.iloc[test_index]\n",
    "        else:\n",
    "            X_train = self.X[train_index]\n",
    "            X_test = self.X[test_index]\n",
    "\n",
    "        y_train = self.y[train_index]\n",
    "        y_test = self.y[test_index]\n",
    "\n",
    "        # Step 2: Prepare LightGBM parameters\n",
    "        # Separate num_round from model parameters\n",
    "        lgb_params = {\n",
    "            'objective': 'binary',  # Binary classification\n",
    "            'metric': 'auc',  # Track AUC during training\n",
    "            'seed': self.rng_seed,\n",
    "            'verbosity': -1  # Suppress output\n",
    "        }\n",
    "\n",
    "        # Extract num_round separately (it's not a model parameter)\n",
    "        num_round = params.get('num_round', 100)  # Default to 100 if not specified\n",
    "        \n",
    "        # Add all other parameters to lgb_params\n",
    "        for param, val in params.items():\n",
    "            if param != 'num_round':\n",
    "                lgb_params[param] = val\n",
    "        \n",
    "        # Step 3: Create LightGBM Dataset objects\n",
    "        # LightGBM's native API requires Dataset objects\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        \n",
    "        # Step 4: Train model using lgb.train (native API)\n",
    "        # This gives more control than the sklearn API\n",
    "        bst = lgb.train(lgb_params, train_data, num_round)\n",
    "        \n",
    "        # Step 5: Predict on test set\n",
    "        # Returns predicted probabilities for binary classification\n",
    "        y_pred_proba = bst.predict(X_test)\n",
    "        \n",
    "        # Step 6: Compute and return loss\n",
    "        # Use the loss_metric provided during initialization\n",
    "        return self.loss_metric(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBMCVObj initialized\n",
      "Number of CV folds: 10\n",
      "Training samples: 2000\n",
      "Features: 25\n"
     ]
    }
   ],
   "source": [
    "# Define loss metric: minimize (1 - AUC)\n",
    "def auc_loss(y_true, y_pred):\n",
    "    return 1 - roc_auc_score(y_true, y_pred)\n",
    "\n",
    "# Create our custom CV objective\n",
    "# Pass all the same arguments as we would to CVObjective\n",
    "cv_obj = LightGBMCVObj(\n",
    "    X=X, \n",
    "    y=y,\n",
    "    loss_metric=auc_loss,  # Our custom loss function\n",
    "    task='classification',\n",
    "    n_splits=10,  # 10-fold cross-validation\n",
    "    stratified=True,  # Use stratified splits to preserve class distribution\n",
    "    rng_seed=42\n",
    ")\n",
    "\n",
    "print(f\"LightGBMCVObj initialized\")\n",
    "print(f\"Number of CV folds: {cv_obj.cv.get_n_splits()}\")\n",
    "print(f\"Training samples: {len(cv_obj.y)}\")\n",
    "print(f\"Features: {cv_obj.X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Custom CV Objective\n",
    "\n",
    "Before running full optimization, test that our custom implementation works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with configuration:\n",
      "Configuration(values={\n",
      "  'colsample_bytree': 0.22360679775,\n",
      "  'learning_rate': 0.0158113883008,\n",
      "  'min_data_in_leaf': 14,\n",
      "  'num_leaves': 16,\n",
      "  'num_round': 224,\n",
      "})\n",
      "\n",
      "CV Loss (1-AUC): 0.072066\n",
      "CV AUC: 0.927934\n",
      "Custom CV objective working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test with the default configuration\n",
    "test_config = config.get_default_configuration()\n",
    "print(\"Testing with configuration:\")\n",
    "print(test_config)\n",
    "\n",
    "# Evaluate this configuration using our custom CV objective\n",
    "# This calls cv_obj.__call__(), which internally:\n",
    "# 1. Loops over all 10 folds\n",
    "# 2. Calls fit_and_test() for each fold\n",
    "# 3. Returns the mean loss across folds\n",
    "test_loss = cv_obj(dict(test_config))\n",
    "test_auc = 1 - test_loss\n",
    "\n",
    "print(f\"\\nCV Loss (1-AUC): {test_loss:.6f}\")\n",
    "print(f\"CV AUC: {test_auc:.6f}\")\n",
    "print(\"Custom CV objective working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Hyperparameter Optimization with FCVOpt\n",
    "\n",
    "Now use the custom CV objective with FCVOpt, exactly as we did with Sklearn CV Obj in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/25 19:26:40 INFO mlflow.tracking.fluent: Experiment with name 'lgb_native_tuning' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of candidates evaluated.....: 50\n",
      "Observed obj at incumbent..........: 0.0386111\n",
      "Estimated obj at incumbent.........: 0.0706391\n",
      "\n",
      " Best Configuration at termination:\n",
      " Configuration(values={\n",
      "  'colsample_bytree': 1.0,\n",
      "  'learning_rate': 0.0383681477115,\n",
      "  'min_data_in_leaf': 6,\n",
      "  'num_leaves': 38,\n",
      "  'num_round': 446,\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Initialize FCVOpt optimizer with our custom CV objective\n",
    "# Note: We use cv_obj.cvloss (deprecated but still supported) or cv_obj directly\n",
    "optimizer = FCVOpt(\n",
    "    obj=cv_obj.cvloss,  # Our custom CV objective's evaluation method\n",
    "    n_folds=cv_obj.cv.get_n_splits(),  # Total number of folds (10)\n",
    "    config=config,  # Search space definition\n",
    "    acq_function='LCB',  # Lower Confidence Bound acquisition function\n",
    "    tracking_dir='./hpt_opt_runs/',  # Directory for saving optimization results\n",
    "    experiment='lgb_native_tuning',  # Experiment name for tracking\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "# Run optimization for 50 BO iterations\n",
    "best_conf = optimizer.optimize(n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-fold CV Loss: 0.0635\n",
      "10-fold CV ROC-AUC: 0.9365\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best configuration found by FCVOpt\n",
    "# Convert loss back to AUC for easier interpretation (loss = 1 - AUC)\n",
    "best_cv_loss = cv_obj(best_conf)\n",
    "best_cv_auc = 1 - best_cv_loss\n",
    "\n",
    "print(f\"10-fold CV Loss: {best_cv_loss:.4f}\")\n",
    "print(f\"10-fold CV ROC-AUC: {best_cv_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcvopt_test",
   "language": "python",
   "name": "fcvopt_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
